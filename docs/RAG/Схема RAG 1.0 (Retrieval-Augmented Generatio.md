Схема RAG (Retrieval-Augmented Generation) в Digital Den построена по принципу «многослойной памяти». Вот как это работает «под капотом»:

1. Слой сохранения (Индексация)
Когда вы сообщаете что-то важное (решение, инсайт или факт), происходит следующее:

Генерация эмбеддинга: Текст отправляется в модель text-embedding-ada-002 (OpenAI), которая превращает его в вектор из 1536 чисел.
Хранилище: Этот вектор сохраняется в базу данных PostgreSQL с расширением pgvector.
Тегирование: Дополнительно извлекаются ключевые темы (топики) для классификации.
2. Слой поиска (Retrieval)
При каждом вашем новом сообщении система делает «запрос в прошлое»:

Гибридный поиск: Мы используем сочетание двух методов:
Семантический поиск: Находим записи, похожие по смыслу (через косинусное расстояние векторов).
Полнотекстовый поиск (FTS): Классический поиск по словам (через tsvector в Postgres) для точных совпадений имён или терминов.
Ранжирование: Результаты смешиваются: 70% веса отдается смыслу, 30% — точным словам.
3. Слой генерации (Augmentation)
Перед тем как LLM (мозг ИИ) получит ваше сообщение, Core Agent собирает «коктейль» контекста:

System Prompt: Ваши правила, стиль общения и текущее состояние Kaizen Engine.
Long-Term Memory: Топ-5 самых релевантных воспоминаний, найденных на шаге 2.
Short-Term Memory: Последние 10 сообщений из Redis («оперативная память» текущего диалога).
User Message: Ваше новое сообщение.
Почему это круто?
Защита от галлюцинаций: Если история пуста, ИИ получает пометку [SYSTEM NOTE], что нужно быть осторожнее с выводами из Long-Term памяти.
Стабильность: Благодаря Redis, Дэн помнит контекст текущей минуты, а благодаря pgvector — факты месячной давности.
