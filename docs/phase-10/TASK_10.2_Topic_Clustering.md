# üè∑Ô∏è TASK 10.2 ‚Äî Topic Auto-Clustering

**–ü—Ä–æ–µ–∫—Ç:** Digital Denis v0.2.0  
**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** –°—Ä–µ–¥–Ω–∏–π  
**–û—Ü–µ–Ω–∫–∞:** 2-3 –¥–Ω—è  
**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** TASK 10.1

---

## üéØ –¶–µ–ª—å

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å memories –≤ —Ç–æ–ø–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏.

---

## üìã –ß–µ–∫–ª–∏—Å—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### Clustering Algorithm
- [x] –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞ (HDBSCAN / K-Means / Agglomerative)
- [x] –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤ HDBSCAN)
- [x] –û–±—Ä–∞–±–æ—Ç–∫–∞ outliers (uncategorized)
- [x] Incremental clustering –¥–ª—è –Ω–æ–≤—ã—Ö items (—á–µ—Ä–µ–∑ orchestrator)

### Topic Generation
- [x] LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏–π —Ç–æ–ø–∏–∫–æ–≤
- [x] –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–ø–∏–∫–∞
- [ ] –ò–µ—Ä–∞—Ä—Ö–∏—è —Ç–æ–ø–∏–∫–æ–≤ (parent-child) - [–û—Ç–ª–æ–∂–µ–Ω–æ]
- [x] Merge –ø–æ—Ö–æ–∂–∏—Ö —Ç–æ–ø–∏–∫–æ–≤ (–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–æ –≤ –ø–∞–π–ø–ª–∞–π–Ω)

### Background Jobs
- [x] Celery task –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
- [x] Cron: –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π/–µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π re-cluster (—á–µ—Ä–µ–∑ Beat)
- [ ] Trigger –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ N –Ω–æ–≤—ã—Ö items - [–í –ø—Ä–æ—Ü–µ—Å—Å–µ]
- [x] Progress reporting (—á–µ—Ä–µ–∑ Celery task status)

### API & UI
- [x] Endpoint `/api/v1/topics/auto-generate`
- [x] Endpoint `/api/v1/topics/{id}/rename` (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π)
- [ ] UI –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (Frontend –±—É–¥–µ—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–π –∑–∞–¥–∞—á–µ)
- [ ] Drag-and-drop –¥–ª—è —Ä—É—á–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏

### Topic Graph
- [x] –°–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–æ–ø–∏–∫–∞–º–∏ (similarity –≤ –ë–î)
- [ ] –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ —Ç–æ–ø–∏–∫–æ–≤
- [ ] Temporal analysis (–∫–∞–∫ —Ç–æ–ø–∏–∫–∏ –º–µ–Ω—è—é—Ç—Å—è)

---

## üì¶ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã

```
backend/
‚îú‚îÄ‚îÄ analytics/
‚îÇ   ‚îú‚îÄ‚îÄ clustering.py           # Clustering algorithms
‚îÇ   ‚îú‚îÄ‚îÄ topic_generator.py      # LLM topic naming
‚îÇ   ‚îî‚îÄ‚îÄ jobs.py                 # Background tasks
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ       ‚îî‚îÄ‚îÄ topics.py           # Topic management API
‚îî‚îÄ‚îÄ memory/
    ‚îî‚îÄ‚îÄ models.py               # + Topic model updates

frontend/
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ components/
    ‚îÇ   ‚îú‚îÄ‚îÄ TopicCloud.tsx      # Topic visualization
    ‚îÇ   ‚îî‚îÄ‚îÄ TopicGraph.tsx      # Graph view
    ‚îî‚îÄ‚îÄ app/
        ‚îî‚îÄ‚îÄ topics/
            ‚îî‚îÄ‚îÄ page.tsx        # Topics page
```

---

## üìù –ü—Ä–∏–º–µ—Ä Clustering Service

```python
# analytics/clustering.py
from sklearn.cluster import HDBSCAN
import numpy as np
from typing import List, Tuple
from uuid import UUID

class ClusteringService:
    def __init__(self, min_cluster_size: int = 5, min_samples: int = 3):
        self.clusterer = HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric='cosine',
            cluster_selection_method='eom'
        )
    
    async def cluster_memories(
        self,
        db: AsyncSession,
        user_id: UUID
    ) -> List[Tuple[int, List[UUID]]]:
        """
        Cluster user's memories and return cluster assignments.
        Returns: [(cluster_id, [memory_ids]), ...]
        """
        # Get all embeddings
        embeddings, memory_ids = await self._get_embeddings(db, user_id)
        
        if len(embeddings) < self.clusterer.min_cluster_size:
            return []
        
        # Fit clusters
        labels = self.clusterer.fit_predict(np.array(embeddings))
        
        # Group by cluster
        clusters = {}
        for i, label in enumerate(labels):
            if label == -1:  # Noise/outlier
                continue
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(memory_ids[i])
        
        return list(clusters.items())
    
    async def generate_topic_name(
        self,
        db: AsyncSession,
        memory_ids: List[UUID]
    ) -> str:
        """Generate topic name using LLM."""
        # Get sample memories
        memories = await self._get_memories(db, memory_ids[:5])
        contents = "\n---\n".join([m.content[:200] for m in memories])
        
        prompt = f"""–î–∞–π –∫–æ—Ä–æ—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (2-4 —Å–ª–æ–≤–∞) –¥–ª—è —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã –∑–∞–ø–∏—Å–µ–π:

{contents}

–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã:"""
        
        from llm.groq import groq
        name = await groq.complete_simple(prompt)
        return name.strip().strip('"')


clustering_service = ClusteringService()
```

---

## üìä Clustering Parameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| min_cluster_size | 5 | Minimum memories per topic |
| min_samples | 3 | Core samples for density |
| metric | cosine | Distance metric |
| cluster_selection_method | eom | Excess of mass (better hierarchy) |

---

## ‚úÖ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è

- [x] –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- [x] –ù–∞–∑–≤–∞–Ω–∏—è —Ç–æ–ø–∏–∫–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- [x] Background job –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
- [ ] UI –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ø–∏–∫–∏ –∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ (Frontend WIP)

---

## üìé –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- [TASK 10.1 ‚Äî Vector DB Integration](./TASK_10.1_Vector_DB.md)
- [TASK 10.3 ‚Äî Analytics Dashboard](./TASK_10.3_Analytics_Dashboard.md)
