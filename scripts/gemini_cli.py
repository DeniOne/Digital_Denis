#!/usr/bin/env python3
"""
Digital Den â€” Gemini CLI
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Console interface for Gemini LLM.
Usage: python scripts/gemini_cli.py "analyze these logs..." [model_name]
"""

import sys
import asyncio
import os

# Add backend to path to allow imports
sys.path.append(os.path.join(os.path.dirname(__file__), "..", "backend"))

from llm.gemini import gemini
from llm.base import LLMMessage


async def main():
    if len(sys.argv) < 2:
        print("Usage: python scripts/gemini_cli.py <prompt> [model]")
        sys.exit(1)
        
    prompt = sys.argv[1]
    model = sys.argv[2] if len(sys.argv) > 2 else None
    
    print(f"ğŸ¤– Gemini {f'({model})' if model else ''} is thinking...")
    print("â”€" * 40)
    
    try:
        response = await gemini.complete_simple(
            prompt=prompt,
            system="Ğ¢Ñ‹ â€” ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Digital Den. "
                   "Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° â€” Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ÑŒ Ğ² ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ (Kaizen). "
                   "ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾.",
            model=model
        )
        
        print(response)
        print("â”€" * 40)
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
