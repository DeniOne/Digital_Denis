"""
Digital Den â€” Memory Agent v2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Extended agent for memory management: saving, aggregation, forgetting, retrieval.
"""

import json
from typing import Optional, List, Dict, Any, Tuple
from uuid import UUID, uuid4
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum

from sqlalchemy import select, func, and_, update, delete
from sqlalchemy.ext.asyncio import AsyncSession

from agents.base import BaseAgent, AgentContext, AgentResponse
from memory.models import MemoryItem, MemoryTopic
from memory.long_term import long_term_memory
from memory.semantic import semantic_memory
from analytics.topics import topic_extractor
from llm.groq import groq
from core.logging import get_logger
from core.audit import AuditService
from core.document_service import document_service

logger = get_logger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Data Classes
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MemoryActionType(Enum):
    SAVE = "save"
    SEARCH = "search"
    FORGET = "forget"
    AGGREGATE = "aggregate"
    RESTORE = "restore"
    INGEST = "ingest"


@dataclass
class MemoryCandidate:
    """Candidate for saving to memory."""
    type: str  # decision, insight, fact, thought
    content: str
    confidence: float
    structured_data: Optional[Dict] = None


@dataclass
class MemoryAction:
    """Action to perform on memory."""
    action_type: MemoryActionType
    item_type: Optional[str] = None
    content: Optional[str] = None
    confidence: float = 0.5
    memory_id: Optional[UUID] = None
    metadata: Dict = field(default_factory=dict)


@dataclass
class ForgetRequest:
    """Request to forget a memory item."""
    memory_id: UUID
    confirmed: bool = False
    related_items: List[UUID] = field(default_factory=list)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Memory Agent v2
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MemoryAgentV2(BaseAgent):
    """
    Memory Agent v2 â€” Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ð¾Ðµ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ.
    
    ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸:
    - auto_save: Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ decisions/insights/facts
    - extract_candidates: Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ
    - forget: ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€ÑƒÐµÐ¼Ð¾Ðµ Ð·Ð°Ð±Ñ‹Ð²Ð°Ð½Ð¸Ðµ Ñ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð¸ÐµÐ¼
    - aggregate: Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ñ…Ð¾Ð¶Ð¸Ñ… ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²
    - search: ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº
    """
    
    name = "memory_v2"
    description = "Extended memory management agent"
    participates_in_dialogue = False
    writes_to_memory = True
    is_synchronous = True
    
    # Confidence thresholds by type
    CONFIDENCE_THRESHOLDS = {
        "decision": 0.85,
        "insight": 0.75,
        "fact": 0.90,
        "thought": 0.50,
    }
    
    # Patterns for detection
    SAVE_PATTERNS = {
        "decision": [
            "Ñ€ÐµÑˆÐ¸Ð»", "Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ", "Ð¿Ñ€Ð¸Ð½ÑÐ» Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ", "Ð±ÑƒÐ´ÐµÐ¼ Ð´ÐµÐ»Ð°Ñ‚ÑŒ",
            "Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÑŽ", "ÑƒÑ‚Ð²ÐµÑ€Ð¶Ð´Ð°ÑŽ", "Ð¾Ð´Ð¾Ð±Ñ€ÑÑŽ", "Ð´ÐµÐ»Ð°ÐµÐ¼ Ñ‚Ð°Ðº"
        ],
        "insight": [
            "Ð¿Ð¾Ð½ÑÐ»", "Ð¾ÑÐ¾Ð·Ð½Ð°Ð»", "Ð¸Ð½ÑÐ°Ð¹Ñ‚", "Ð²Ñ‹Ð²Ð¾Ð´", "ÐºÐ»ÑŽÑ‡ÐµÐ²Ð¾Ð¹ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚",
            "Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÑŽ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ", "Ð²Ð°Ð¶Ð½Ð¾ Ñ‡Ñ‚Ð¾", "Ð¾ÑÐ¾Ð·Ð½Ð°Ð½Ð¸Ðµ"
        ],
        "fact": [
            "Ñ„Ð°ÐºÑ‚:", "Ð´Ð°Ð½Ð½Ñ‹Ðµ:", "ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°:",
        ],
    }
    
    async def process(self, context: AgentContext) -> AgentResponse:
        """Process memory-related request."""
        
        # Classify the operation
        operation = self._classify_operation(context.user_message)
        
        if operation == MemoryActionType.SEARCH:
            return await self._handle_search(context)
        elif operation == MemoryActionType.FORGET:
            return await self._handle_forget(context)
        elif operation == MemoryActionType.AGGREGATE:
            return await self._handle_aggregate(context)
        elif operation == MemoryActionType.INGEST:
            return await self._handle_ingest(context)
        else:
            return await self._handle_save(context)
    
    def _classify_operation(self, message: str) -> MemoryActionType:
        """Classify the memory operation from message."""
        message_lower = message.lower()
        
        forget_keywords = ["Ð·Ð°Ð±ÑƒÐ´ÑŒ", "ÑƒÐ´Ð°Ð»Ð¸", "Ð·Ð°Ð±Ñ‹Ñ‚ÑŒ", "ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ", "ÑƒÐ±ÐµÑ€Ð¸"]
        search_keywords = ["Ð½Ð°Ð¹Ð´Ð¸", "Ð²ÑÐ¿Ð¾Ð¼Ð½Ð¸", "Ñ‡Ñ‚Ð¾ Ñ", "ÐºÐ¾Ð³Ð´Ð° Ñ"]
        aggregate_keywords = ["Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½Ð¸", "Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€ÑƒÐ¹", "ÑÐ³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐ¹"]
        ingest_keywords = ["Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚", "Ð¼Ð°Ð½Ð¸Ñ„ÐµÑÑ‚", "ÑÑ‚Ð°Ñ‚ÑŒÑ", "Ñ‚ÐµÐºÑÑ‚ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°", "Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ð¹ Ð¸ Ð·Ð°Ð¿Ð¾Ð¼Ð½Ð¸"]
        save_keywords = ["Ð·Ð°Ð¿Ð¾Ð¼Ð½Ð¸", "ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸", "Ð·Ð°Ð¿Ð¸ÑˆÐ¸"]
        
        # Ingest takes priority if text is long or contains keywords
        if len(message) > 1000 or any(kw in message_lower for kw in ingest_keywords):
            return MemoryActionType.INGEST
            
        for kw in forget_keywords:
            if kw in message_lower:
                return MemoryActionType.FORGET
        
        for kw in search_keywords:
            if kw in message_lower:
                return MemoryActionType.SEARCH
        
        for kw in aggregate_keywords:
            if kw in message_lower:
                return MemoryActionType.AGGREGATE
        
        return MemoryActionType.SAVE
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Auto-save functionality
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def auto_save(
        self,
        db: AsyncSession,
        agent_response: AgentResponse,
        context: AgentContext,
        user_id: Optional[UUID] = None,
    ) -> List[MemoryAction]:
        """
        Automatically save worthy items from agent response.
        Called after other agents respond.
        
        Returns list of memory actions performed.
        """
        actions = []
        
        # 1. Extract candidates from response
        candidates = await self.extract_candidates(
            context.user_message + "\n" + agent_response.content
        )
        
        # 2. Filter by confidence threshold
        worthy = []
        for candidate in candidates:
            threshold = self.CONFIDENCE_THRESHOLDS.get(candidate.type, 0.5)
            if candidate.confidence >= threshold:
                worthy.append(candidate)
        
        # 3. Check for duplicates using semantic similarity
        unique = await self._deduplicate(db, worthy, user_id=user_id)
        
        # 4. Save each unique item
        for candidate in unique:
            item = await long_term_memory.save(
                db=db,
                item_type=candidate.type,
                content=candidate.content,
                summary=await self._generate_summary(candidate),
                structured_data=candidate.structured_data,
                source_agent=agent_response.agent,
                source_session=context.session_id,
                confidence=candidate.confidence,
                user_id=user_id,
            )
            
            # Audit log
            await AuditService.log_action(
                db=db,
                user_id=user_id,
                action="memory_save",
                target_type="memory_item",
                target_id=str(item.id),
                changes={"content": candidate.content, "type": candidate.type},
                meta_data={"confidence": candidate.confidence, "source": "auto_save"}
            )
            
            # Index for semantic search
            await semantic_memory.index(db, item.id, candidate.content)
            
            # Extract and assign topics
            topics = await topic_extractor.extract(candidate.content)
            for topic in topics:
                if topic.topic_id:
                    await self._assign_topic(db, item.id, topic.topic_id, topic.confidence)
            
            actions.append(MemoryAction(
                action_type=MemoryActionType.SAVE,
                item_type=candidate.type,
                content=candidate.content[:100],
                confidence=candidate.confidence,
                memory_id=item.id,
            ))
        
        await db.commit()
        return actions
    
    async def extract_candidates(self, text: str) -> List[MemoryCandidate]:
        """
        Extract memory candidates from text using LLM.
        """
        prompt = f"""ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ñ‚ÐµÐºÑÑ‚ Ð¸ Ð½Ð°Ð¹Ð´Ð¸:
1. Decisions â€” ÑÐ²Ð½Ñ‹Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ ("Ñ€ÐµÑˆÐ¸Ð»", "Ð±ÑƒÐ´ÐµÐ¼ Ð´ÐµÐ»Ð°Ñ‚ÑŒ")
2. Insights â€” Ð¾Ð·Ð°Ñ€ÐµÐ½Ð¸Ñ, Ð²Ñ‹Ð²Ð¾Ð´Ñ‹ ("Ð¿Ð¾Ð½ÑÐ»", "Ð¾ÑÐ¾Ð·Ð½Ð°Ð»")
3. Facts â€” ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹, Ñ‡Ð¸ÑÐ»Ð°, Ð´Ð°Ñ‚Ñ‹

Ð¢ÐµÐºÑÑ‚:
{text[:1000]}

Ð’ÐµÑ€Ð½Ð¸ JSON Ð¼Ð°ÑÑÐ¸Ð²:
[{{"type": "decision|insight|fact", "content": "Ñ‚ÐµÐºÑÑ‚", "confidence": 0.0-1.0}}]

Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ð¾, Ñ‡Ñ‚Ð¾ ÑÐ²Ð½Ð¾ Ð¿Ñ€Ð¸ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚. Ð•ÑÐ»Ð¸ Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð½ÐµÑ‚ â€” Ð²ÐµÑ€Ð½Ð¸ [].
JSON:"""
        
        try:
            result = await groq.complete_simple(prompt)
            return self._parse_candidates(result)
        except Exception as e:
            # Fallback: rule-based extraction
            return self._rule_based_extraction(text)
    
    def _parse_candidates(self, response: str) -> List[MemoryCandidate]:
        """Parse LLM response to candidates."""
        try:
            # Clean response
            response = response.strip()
            if response.startswith("```"):
                response = response.split("```")[1]
                if response.startswith("json"):
                    response = response[4:]
            
            data = json.loads(response)
            
            candidates = []
            for item in data:
                if isinstance(item, dict):
                    candidates.append(MemoryCandidate(
                        type=item.get("type", "thought"),
                        content=item.get("content", ""),
                        confidence=float(item.get("confidence", 0.5)),
                    ))
            
            return candidates
            
        except (json.JSONDecodeError, ValueError):
            return []
    
    def _rule_based_extraction(self, text: str) -> List[MemoryCandidate]:
        """Fallback rule-based extraction."""
        candidates = []
        text_lower = text.lower()
        
        for item_type, patterns in self.SAVE_PATTERNS.items():
            for pattern in patterns:
                if pattern in text_lower:
                    # Extract sentence containing pattern
                    idx = text_lower.find(pattern)
                    start = max(0, text[:idx].rfind('.') + 1)
                    end = text.find('.', idx)
                    if end == -1:
                        end = len(text)
                    
                    content = text[start:end].strip()
                    if content:
                        candidates.append(MemoryCandidate(
                            type=item_type,
                            content=content,
                            confidence=0.7,  # Medium confidence for rule-based
                        ))
                    break  # Only one candidate per type
        
        return candidates
    
    async def _deduplicate(
        self, 
        db: AsyncSession, 
        candidates: List[MemoryCandidate],
        user_id: Optional[UUID] = None,
    ) -> List[MemoryCandidate]:
        """Remove duplicates using semantic similarity."""
        unique = []
        
        for candidate in candidates:
            # Search for similar existing items
            similar = await semantic_memory.search(
                db=db,
                query=candidate.content,
                limit=1,
                similarity_threshold=0.85,  # High threshold for duplicate detection
            )
            
            if not similar:
                unique.append(candidate)
        
        return unique
    
    async def _generate_summary(self, candidate: MemoryCandidate) -> str:
        """Generate short summary for memory item."""
        type_labels = {
            "decision": "Ð ÐµÑˆÐµÐ½Ð¸Ðµ",
            "insight": "Ð˜Ð½ÑÐ°Ð¹Ñ‚",
            "fact": "Ð¤Ð°ÐºÑ‚",
            "thought": "ÐœÑ‹ÑÐ»ÑŒ",
        }
        label = type_labels.get(candidate.type, "Ð—Ð°Ð¿Ð¸ÑÑŒ")
        return f"[{label}] {candidate.content[:100]}..."
    
    async def _assign_topic(
        self, 
        db: AsyncSession, 
        memory_id: UUID, 
        topic_id: UUID,
        confidence: float
    ) -> None:
        """Assign topic to memory item."""
        mt = MemoryTopic(
            memory_id=memory_id,
            topic_id=topic_id,
            confidence=confidence,
            assigned_by="llm",
        )
        db.add(mt)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Forget functionality
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def prepare_forget(
        self,
        db: AsyncSession,
        query: str,
    ) -> Tuple[Optional[ForgetRequest], str]:
        """
        Prepare to forget: find item and related items.
        Returns ForgetRequest and message for confirmation.
        """
        # Search for matching item
        results = await semantic_memory.search(db, query, limit=1)
        
        if not results:
            return None, "ÐÐµ Ð½Ð°ÑˆÑ‘Ð» Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰ÐµÐ³Ð¾ Ð²Ð¾ÑÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ñ Ð´Ð»Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ."
        
        memory, similarity = results[0]
        
        # Find related items
        related = await semantic_memory.find_similar(db, memory.id, limit=5)
        related_ids = [r[0].id for r in related if r[1] > 0.7]
        
        request = ForgetRequest(
            memory_id=memory.id,
            related_items=related_ids,
        )
        
        # Build confirmation message
        message = f"""ÐÐ°Ð¹Ð´ÐµÐ½Ð¾: "{memory.content[:100]}..."
Ð¢Ð¸Ð¿: {memory.item_type}
Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾: {memory.created_at}

âš ï¸ Ð¡Ð²ÑÐ·Ð°Ð½Ð¾ Ñ {len(related_ids)} Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ Ð·Ð°Ð¿Ð¸ÑÑÐ¼Ð¸.

ÐŸÐ¾Ð´Ñ‚Ð²ÐµÑ€Ð´Ð¸Ñ‚Ðµ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ? Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ Ð² Ñ‚ÐµÑ‡ÐµÐ½Ð¸Ðµ 30 Ð´Ð½ÐµÐ¹."""
        
        return request, message
    
    async def execute_forget(
        self,
        db: AsyncSession,
        request: ForgetRequest,
    ) -> bool:
        """Execute forget: soft delete the item."""
        if not request.confirmed:
            return False
        
        # Soft delete (set status to 'deleted')
        result = await db.execute(
            update(MemoryItem)
            .where(MemoryItem.id == request.memory_id)
            .values(
                status="deleted",
                updated_at=datetime.utcnow(),
            )
        )
        
        # Audit log
        await AuditService.log_action(
            db=db,
            user_id=None, # user_id needs to be passed in Context or Request
            action="memory_forget",
            target_type="memory_item",
            target_id=str(request.memory_id),
            changes={"status": {"old": "active", "new": "deleted"}}
        )
        
        await db.commit()
        return result.rowcount > 0
    
    async def restore(
        self,
        db: AsyncSession,
        memory_id: UUID,
    ) -> bool:
        """Restore a deleted memory item (within 30 days)."""
        result = await db.execute(
            update(MemoryItem)
            .where(
                and_(
                    MemoryItem.id == memory_id,
                    MemoryItem.status == "deleted",
                )
            )
            .values(
                status="active",
                updated_at=datetime.utcnow(),
            )
        )
        
        await db.commit()
        return result.rowcount > 0
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Aggregate functionality
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def aggregate_similar(
        self,
        db: AsyncSession,
        topic_id: Optional[UUID] = None,
        similarity_threshold: float = 0.85,
        min_cluster_size: int = 3,
    ) -> List[UUID]:
        """
        Aggregate similar memory items into summaries.
        
        Returns list of created aggregation IDs.
        """
        # Get active items (optionally filtered by topic)
        query = select(MemoryItem).where(MemoryItem.status == "active")
        
        if topic_id:
            query = query.join(MemoryTopic).where(MemoryTopic.topic_id == topic_id)
        
        result = await db.execute(query)
        items = result.scalars().all()
        
        if len(items) < min_cluster_size:
            return []
        
        # Cluster by similarity
        clusters = await self._cluster_items(db, items, similarity_threshold)
        
        aggregated_ids = []
        for cluster in clusters:
            if len(cluster) >= min_cluster_size:
                # Generate summary
                summary = await self._generate_cluster_summary(cluster)
                
                # Create aggregation item
                aggregation = MemoryItem(
                    id=uuid4(),
                    user_id=cluster[0].user_id,
                    item_type="aggregation",
                    content=summary,
                    structured_data={
                        "source_ids": [str(i.id) for i in cluster],
                        "aggregated_at": datetime.utcnow().isoformat(),
                    },
                    status="active",
                    confidence=0.9,
                )
                db.add(aggregation)
                aggregated_ids.append(aggregation.id)
                
                # Mark originals as aggregated
                for item in cluster:
                    item.status = "aggregated"
        
        await db.commit()
        return aggregated_ids
    
    async def _cluster_items(
        self,
        db: AsyncSession,
        items: List[MemoryItem],
        threshold: float,
    ) -> List[List[MemoryItem]]:
        """Cluster items by semantic similarity."""
        clusters = []
        used = set()
        
        for item in items:
            if item.id in used:
                continue
            
            # Find similar items
            similar = await semantic_memory.find_similar(db, item.id, limit=10)
            
            cluster = [item]
            for similar_item, similarity in similar:
                if similarity >= threshold and similar_item.id not in used:
                    cluster.append(similar_item)
                    used.add(similar_item.id)
            
            if len(cluster) > 1:
                clusters.append(cluster)
                used.add(item.id)
        
        return clusters
    
    async def _generate_cluster_summary(self, items: List[MemoryItem]) -> str:
        """Generate summary for a cluster of items."""
        contents = "\n---\n".join([i.content[:200] for i in items[:5]])
        
        prompt = f"""Ð¡Ð¾Ð·Ð´Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ñ€ÐµÐ·ÑŽÐ¼Ðµ (2-3 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ) Ð´Ð»Ñ ÑÑ‚Ð¸Ñ… ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹:

{contents}

Ð ÐµÐ·ÑŽÐ¼Ðµ:"""
        
        try:
            result = await groq.complete_simple(prompt)
            return result.strip()
        except Exception:
            return f"ÐÐ³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ñ {len(items)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð¾Ñ‚ {items[0].created_at}"
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Search & Context
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def get_context_memories(
        self,
        db: AsyncSession,
        user_message: str,
        limit: int = 5,
        user_id: Optional[UUID] = None,
    ) -> List[Dict]:
        """Get relevant memories for context using semantic search."""
        
        # Try semantic search first
        results = await semantic_memory.search(
            db=db,
            query=user_message,
            limit=limit,
            user_id=user_id,
            similarity_threshold=0.6,
        )
        
        if results:
            return [
                {
                    "id": str(m.id),
                    "item_type": m.item_type,
                    "content": m.content,
                    "summary": m.summary,
                    "confidence": m.confidence,
                    "similarity": score,
                    "created_at": m.created_at.isoformat() if m.created_at else None,
                }
                for m, score in results
            ]
        
        # Fallback to text search
        memories = await long_term_memory.search(
            db=db,
            user_id=user_id,
            query_text=user_message[:100],
            limit=limit,
        )
        
        return [
            {
                "id": str(m.id),
                "item_type": m.item_type,
                "content": m.content,
                "summary": m.summary,
                "created_at": m.created_at.isoformat() if m.created_at else None,
            }
            for m in memories
        ]
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Operation Handlers
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def _handle_search(self, context: AgentContext) -> AgentResponse:
        """Handle search operation."""
        return AgentResponse(
            content="ÐŸÐ¾Ð¸ÑÐº Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸...",
            agent=self.name,
            save_to_memory=False,
        )
    
    async def _handle_forget(self, context: AgentContext) -> AgentResponse:
        """Handle forget operation."""
        return AgentResponse(
            content="ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ðº ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸ÑŽ...",
            agent=self.name,
            save_to_memory=False,
            metadata={"requires_confirmation": True},
        )
    
    async def _handle_aggregate(self, context: AgentContext) -> AgentResponse:
        """Handle aggregate operation."""
        return AgentResponse(
            content="ÐÐ³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°. Ð¯ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½Ð¸Ð» Ð¿Ð¾Ñ…Ð¾Ð¶Ð¸Ðµ Ð²Ð¾ÑÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ñ Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹.",
            agent=self.name,
            save_to_memory=False,
        )

    async def _handle_save(self, context: AgentContext) -> AgentResponse:
        """Handle explicit save operation."""
        # The actual saving will be handled by auto_save after this response
        return AgentResponse(
            content="ÐŸÑ€Ð¸Ð½ÑÑ‚Ð¾! Ð¯ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð» Ð²Ð°ÑˆÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ð» Ð²Ð°Ð¶Ð½Ñ‹Ðµ Ð´ÐµÑ‚Ð°Ð»Ð¸ Ð² Ð´Ð¾Ð»Ð³Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ Ð¿Ð°Ð¼ÑÑ‚ÑŒ. âœ…",
            agent=self.name,
            save_to_memory=True,
        )

    async def _handle_ingest(self, context: AgentContext) -> AgentResponse:
        """Handle document ingestion via chat."""
        db = context.db
        user_id = context.user_id
        text = context.user_message
        
        # Try to find a title in the first line
        lines = text.split("\n")
        title = "Ð§Ð°Ñ‚-Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚"
        if len(lines) > 0 and len(lines[0]) < 100:
            potential_title = lines[0].strip().rstrip(":")
            if any(kw in potential_title.lower() for kw in ["Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚", "Ð¼Ð°Ð½Ð¸Ñ„ÐµÑÑ‚", "Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ", "Ñ„Ð°Ð¹Ð»"]):
                title = potential_title
                text = "\n".join(lines[1:])
        
        try:
            chunks_count = await document_service.ingest_text(
                db=db,
                user_id=user_id,
                text=text,
                title=title,
                source_type="chat_upload"
            )
            
            return AgentResponse(
                content=f"ðŸ“¥ Ð¯ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð» Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ð» Ð²Ð°Ñˆ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ Â«{title}Â» Ð² Ð±Ð°Ð·Ñƒ Ð·Ð½Ð°Ð½Ð¸Ð¹.\n"
                        f"Ð¢ÐµÐºÑÑ‚ Ñ€Ð°Ð·Ð±Ð¸Ñ‚ Ð½Ð° {chunks_count} Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ° Ð² Ð±ÑƒÐ´ÑƒÑ‰ÐµÐ¼. âœ…",
                agent=self.name,
                save_to_memory=False,
            )
        except Exception as e:
            logger.error("chat_ingest_error", error=str(e))
            return AgentResponse(
                content="âŒ Ðš ÑÐ¾Ð¶Ð°Ð»ÐµÐ½Ð¸ÑŽ, Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ ÐµÐ³Ð¾ Ñ‡Ð°ÑÑ‚ÑÐ¼Ð¸.",
                agent=self.name,
                save_to_memory=False,
            )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Global instances - keep both for backward compatibility
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Legacy instance
from agents.base import BaseAgent as _BaseAgent

class _LegacyMemoryAgent(_BaseAgent):
    """Legacy Memory Agent for backward compatibility."""
    
    name = "memory"
    description = "Memory management agent"
    participates_in_dialogue = False
    writes_to_memory = True
    is_synchronous = True
    
    async def process(self, context):
        return AgentResponse(
            content="Memory operation completed",
            agent=self.name,
            save_to_memory=False,
        )
    
    async def save_from_response(self, db, response, session_id, user_message, user_id=None):
        """Delegate to v2."""
        v2 = MemoryAgentV2()
        actions = await v2.auto_save(db, response, 
            AgentContext(session_id=session_id, user_message=user_message), user_id=user_id)
        return actions[0].memory_id if actions else None
    
    async def get_context_memories(self, db, user_message, limit=5, user_id=None):
        """Delegate to v2."""
        v2 = MemoryAgentV2()
        return await v2.get_context_memories(db, user_message, limit, user_id)


memory_agent = _LegacyMemoryAgent()
memory_agent_v2 = MemoryAgentV2()
